{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccffec8-ed24-4da3-8930-cddf75527685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jihad\\PycharmProjects\\energyinfo\\venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,365</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │             \u001b[38;5;34m320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)                  │           \u001b[38;5;34m1,365\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,685</span> (6.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,685\u001b[0m (6.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,685</span> (6.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,685\u001b[0m (6.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8977 - val_loss: 0.7846\n",
      "Epoch 2/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 909us/step - loss: 0.8530 - val_loss: 0.7614\n",
      "Epoch 3/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 851us/step - loss: 0.8301 - val_loss: 0.7491\n",
      "Epoch 4/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 942us/step - loss: 0.8267 - val_loss: 0.7396\n",
      "Epoch 5/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.7634 - val_loss: 0.7287\n",
      "Epoch 6/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.8192 - val_loss: 0.7216\n",
      "Epoch 7/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.7722 - val_loss: 0.7165\n",
      "Epoch 8/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.7479 - val_loss: 0.7102\n",
      "Epoch 9/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.7752 - val_loss: 0.7024\n",
      "Epoch 10/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.7891 - val_loss: 0.6960\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 550us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jihad\\PycharmProjects\\energyinfo\\venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,071</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m11,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)                  │           \u001b[38;5;34m1,071\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,071</span> (47.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,071\u001b[0m (47.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,071</span> (47.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,071\u001b[0m (47.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 871us/step - loss: 0.8932 - val_loss: 0.7863\n",
      "Epoch 2/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 848us/step - loss: 0.8801 - val_loss: 0.7620\n",
      "Epoch 3/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 865us/step - loss: 0.8604 - val_loss: 0.7504\n",
      "Epoch 4/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 902us/step - loss: 0.8483 - val_loss: 0.7401\n",
      "Epoch 5/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 967us/step - loss: 0.7592 - val_loss: 0.7319\n",
      "Epoch 6/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 878us/step - loss: 0.7930 - val_loss: 0.7218\n",
      "Epoch 7/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 848us/step - loss: 0.7968 - val_loss: 0.7146\n",
      "Epoch 8/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 866us/step - loss: 0.7623 - val_loss: 0.7064\n",
      "Epoch 9/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 871us/step - loss: 0.7786 - val_loss: 0.7004\n",
      "Epoch 10/10\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 862us/step - loss: 0.7588 - val_loss: 0.6929\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 523us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('TrainData_A.csv')\n",
    "aggregated_load = pd.read_csv('AggregatedLoad_A.csv')\n",
    "test_data = pd.read_csv('TestData_A.csv')\n",
    "template = pd.read_csv('DisaggregatedLoad_Template.csv')\n",
    "\n",
    "# Rename columns for convenience\n",
    "train_data.columns = ['index', 'aggregated_load'] + [f'appliance_{i}' for i in range(1, 22)]\n",
    "aggregated_load.columns = ['index', 'aggregated_load']\n",
    "test_data.columns = ['index', 'aggregated_load'] + [f'appliance_{i}' for i in range(1, 22)]\n",
    "template.columns = ['index'] + [f'appliance_{i}_pred' for i in range(1, 22)]\n",
    "\n",
    "# Add rolling mean feature to train, test, and aggregated_load datasets\n",
    "window_size = 10\n",
    "\n",
    "train_data['agg_load_mean'] = train_data['aggregated_load'].rolling(window=window_size, min_periods=1).mean()\n",
    "test_data['agg_load_mean'] = test_data['aggregated_load'].rolling(window=window_size, min_periods=1).mean()\n",
    "aggregated_load['agg_load_mean'] = aggregated_load['aggregated_load'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "def calculate_frequency(data, window_size):\n",
    "    # Find local maxima and minima\n",
    "    peaks, _ = find_peaks(data)\n",
    "    troughs, _ = find_peaks(-data)\n",
    "    \n",
    "    # Combine and sort indices of peaks and troughs\n",
    "    extrema = np.sort(np.concatenate([peaks, troughs]))\n",
    "    \n",
    "    # Calculate frequency of extrema within the window\n",
    "    frequency = np.zeros_like(data)\n",
    "    for i in range(len(data)):\n",
    "        start = max(0, i - window_size)\n",
    "        end = i\n",
    "        frequency[i] = np.sum((extrema >= start) & (extrema < end))\n",
    "    \n",
    "    return frequency\n",
    "\n",
    "# Add frequency feature to train, test, and aggregated_load datasets\n",
    "train_data['agg_load_freq'] = calculate_frequency(train_data['aggregated_load'].values, window_size)\n",
    "test_data['agg_load_freq'] = calculate_frequency(test_data['aggregated_load'].values, window_size)\n",
    "aggregated_load['agg_load_freq'] = calculate_frequency(aggregated_load['aggregated_load'].values, window_size)\n",
    "\n",
    "# Add lagged aggregated load feature\n",
    "train_data['lagged_agg_load'] = train_data['aggregated_load'].shift(1)\n",
    "test_data['lagged_agg_load'] = test_data['aggregated_load'].shift(1)\n",
    "aggregated_load['lagged_agg_load'] = aggregated_load['aggregated_load'].shift(1)\n",
    "\n",
    "# Drop the first row as it will have NaN values due to shifting\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "aggregated_load.dropna(inplace=True)\n",
    "\n",
    "# Normalize the data for aggregated load, its mean, frequency, and lagged aggregated load separately\n",
    "scaler_agg = StandardScaler()\n",
    "scaler_agg_mean = StandardScaler()\n",
    "scaler_agg_freq = StandardScaler()\n",
    "scaler_agg_lagged = StandardScaler()\n",
    "scaler_appliances = StandardScaler()\n",
    "\n",
    "# Fit scaler on the aggregated load, its mean, frequency, and lagged aggregated load from training data\n",
    "train_agg_normalized = scaler_agg.fit_transform(train_data[['aggregated_load']])\n",
    "train_agg_mean_normalized = scaler_agg_mean.fit_transform(train_data[['agg_load_mean']])\n",
    "train_agg_freq_normalized = scaler_agg_freq.fit_transform(train_data[['agg_load_freq']])\n",
    "train_lagged_normalized = scaler_agg_lagged.fit_transform(train_data[['lagged_agg_load']])\n",
    "\n",
    "# Fit scaler on the appliance loads from training data\n",
    "train_appliances_normalized = scaler_appliances.fit_transform(train_data.iloc[:, 2:-3])  # Exclude agg_load_mean, agg_load_freq, and lagged_agg_load\n",
    "\n",
    "# Concatenate the normalized features\n",
    "train_features_normalized = np.concatenate([\n",
    "    train_agg_normalized, train_agg_mean_normalized, train_agg_freq_normalized, train_lagged_normalized\n",
    "], axis=1)\n",
    "test_features_normalized = np.concatenate([\n",
    "    scaler_agg.transform(test_data[['aggregated_load']]),\n",
    "    scaler_agg_mean.transform(test_data[['agg_load_mean']]),\n",
    "    scaler_agg_freq.transform(test_data[['agg_load_freq']]),\n",
    "    scaler_agg_lagged.transform(test_data[['lagged_agg_load']])\n",
    "], axis=1)\n",
    "aggregated_features_normalized = np.concatenate([\n",
    "    scaler_agg.transform(aggregated_load[['aggregated_load']]),\n",
    "    scaler_agg_mean.transform(aggregated_load[['agg_load_mean']]),\n",
    "    scaler_agg_freq.transform(aggregated_load[['agg_load_freq']]),\n",
    "    scaler_agg_lagged.transform(aggregated_load[['lagged_agg_load']])\n",
    "], axis=1)\n",
    "\n",
    "# Split data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_features_normalized, train_appliances_normalized, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the training and test data\n",
    "X_train = X_train.reshape(-1, 1, 4)  # Include 4 features: aggregated load, its mean, its frequency, and lagged aggregated load\n",
    "X_test = X_test.reshape(-1, 1, 4)\n",
    "X_aggregated = aggregated_features_normalized.reshape(-1, 1, 4)\n",
    "\n",
    "# CNN Model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(1, 4)))  # Update input_shape to (1, 4)\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(21, activation='linear'))\n",
    "\n",
    "cnn_model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "cnn_model.summary()\n",
    "\n",
    "# CNN Training\n",
    "cnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predicting with CNN\n",
    "y_pred_cnn_test = cnn_model.predict(X_test)\n",
    "y_pred_cnn_full = cnn_model.predict(X_aggregated)\n",
    "\n",
    "# LSTM Model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(50, activation='relu', input_shape=(1, 4)))  # Update input_shape to (1, 4)\n",
    "lstm_model.add(Dense(21))\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "lstm_model.summary()\n",
    "\n",
    "# LSTM Training\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predicting with LSTM\n",
    "y_pred_lstm_test = lstm_model.predict(X_test)\n",
    "y_pred_lstm_full = lstm_model.predict(X_aggregated)\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train.reshape(-1, 4), y_train)  # Reshape X_train for RandomForest\n",
    "\n",
    "# Predicting with Random Forest\n",
    "y_pred_rf_test = rf_model.predict(X_test.reshape(-1, 4))\n",
    "y_pred_rf_full = rf_model.predict(X_aggregated.reshape(-1, 4))\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def calculate_rmse(true_values, pred_values):\n",
    "    true_values_scaled = true_values / true_values.max(axis=0)\n",
    "    pred_values_scaled = pred_values / true_values.max(axis=0)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values_scaled, pred_values_scaled, multioutput='raw_values'))\n",
    "    return rmse\n",
    "\n",
    "# Calculate RMSE for each appliance\n",
    "rmse_cnn = calculate_rmse(y_test, y_pred_cnn_test)\n",
    "rmse_lstm = calculate_rmse(y_test, y_pred_lstm_test)\n",
    "rmse_rf = calculate_rmse(y_test, y_pred_rf_test)\n",
    "\n",
    "# Combine RMSE into a DataFrame for comparison\n",
    "rmse_df = pd.DataFrame({\n",
    "    'appliance': [f'appliance_{i}' for i in range(1, 22)],\n",
    "    'rmse_cnn': rmse_cnn,\n",
    "    'rmse_lstm': rmse_lstm,\n",
    "    'rmse_rf': rmse_rf\n",
    "})\n",
    "\n",
    "# Determine the best model for each appliance\n",
    "best_models = rmse_df[['rmse_cnn', 'rmse_lstm', 'rmse_rf']].idxmin(axis=1)\n",
    "best_models = best_models.replace({'rmse_cnn': 'CNN', 'rmse_lstm': 'LSTM', 'rmse_rf': 'RF'})\n",
    "rmse_df['best_model'] = best_models\n",
    "\n",
    "# Initialize the hybrid predictions array for test data\n",
    "hybrid_predictions_test = np.zeros_like(y_pred_cnn_test)\n",
    "\n",
    "# Assign the best model's predictions to the hybrid model for test data\n",
    "for i in range(21):\n",
    "    if rmse_df.loc[i, 'best_model'] == 'CNN':\n",
    "        hybrid_predictions_test[:, i] = y_pred_cnn_test[:, i]\n",
    "    elif rmse_df.loc[i, 'best_model'] == 'LSTM':\n",
    "        hybrid_predictions_test[:, i] = y_pred_lstm_test[:, i]\n",
    "    else:\n",
    "        hybrid_predictions_test[:, i] = y_pred_rf_test[:, i]\n",
    "\n",
    "# Calculate and show summed RMSE for the hybrid model on test data\n",
    "rmse_hybrid_test = calculate_rmse(y_test, hybrid_predictions_test)\n",
    "print(\"Hybrid Model RMSE Sum on Test Data:\", rmse_hybrid_test)\n",
    "\n",
    "# Initialize the hybrid predictions array for AggregatedLoad_A\n",
    "hybrid_predictions_full = np.zeros_like(y_pred_cnn_full)\n",
    "\n",
    "# Assign the best model's predictions to the hybrid model for AggregatedLoad_A\n",
    "for i in range(21):\n",
    "    if rmse_df.loc[i, 'best_model'] == 'CNN':\n",
    "        hybrid_predictions_full[:, i] = y_pred_cnn_full[:, i]\n",
    "    elif rmse_df.loc[i, 'best_model'] == 'LSTM':\n",
    "        hybrid_predictions_full[:, i] = y_pred_lstm_full[:, i]\n",
    "    else:\n",
    "        hybrid_predictions_full[:, i] = y_pred_rf_full[:, i]\n",
    "\n",
    "# Save predictions to CSV\n",
    "def save_predictions(predictions, filename):\n",
    "    predictions_scaled = scaler_appliances.inverse_transform(predictions)\n",
    "    template_copy = template.copy()\n",
    "    if len(predictions_scaled) < len(template_copy):\n",
    "        # Extend predictions_scaled to match the length of template_copy\n",
    "        diff = len(template_copy) - len(predictions_scaled)\n",
    "        predictions_scaled = np.concatenate([predictions_scaled, np.zeros((diff, predictions_scaled.shape[1]))])\n",
    "    template_copy.iloc[:, 1:] = predictions_scaled\n",
    "    template_copy.to_csv(filename, index=False)\n",
    "    return predictions_scaled\n",
    "\n",
    "save_predictions(y_pred_cnn_full[:-1], 'Predicted_CNN.csv')\n",
    "save_predictions(y_pred_lstm_full[:-1], 'Predicted_LSTM.csv')\n",
    "save_predictions(y_pred_rf_full[:-1], 'Predicted_RF.csv')\n",
    "hybrid_predictions_full=save_predictions(hybrid_predictions_full[:-1], 'Predicted_Hybrid.csv')\n",
    "\n",
    "\n",
    "# Load ground truth data for AggregatedLoad_A\n",
    "true_A = pd.read_csv(\"TestData_A.csv\", index_col=0)\n",
    "true_A = true_A.iloc[:, 1:]  # Ignore aggregate load column\n",
    "true_A_values_scaled = true_A.values / true_A.max(axis=0).values\n",
    "\n",
    "# Calculate and show summed RMSE for the hybrid model on AggregatedLoad_A\n",
    "pred_A_values_scaled = hybrid_predictions_full / true_A.max(axis=0).values\n",
    "rmse_sum_hybrid = np.sum(mean_squared_error(true_A_values_scaled, pred_A_values_scaled, multioutput=\"raw_values\"))\n",
    "print(\"Hybrid Model RMSE Sum on AggregatedLoad_A:\", rmse_sum_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a847d5-f89c-4fa0-bb69-8a248db8d0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
